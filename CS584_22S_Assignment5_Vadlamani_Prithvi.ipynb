{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2R_WD09bz8C"
      },
      "source": [
        "# CS 584 Assignment 5 -- Dependency Parsing\n",
        "\n",
        "#### Name: Prithvi Vadlamani\n",
        "#### Stevens ID: 10476457"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmpBhfpib-XF"
      },
      "source": [
        "## In this assignment, you are required to follow the steps below:\n",
        "1. Review the lecture slides.\n",
        "2. implementing a neural-network based dependency parser with the goal of maximizing performance on the UAS (Unlabeled Attachment Score) metric\n",
        "\n",
        "In this assignment, we will use Tensorflow.\n",
        "\n",
        "```console\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
        "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
        "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h4rSEOjhLLG9",
        "outputId": "884a0a5f-6ab9-4a29-f037-4051a792c7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (3.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (2.9.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (7.9.0)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (6.5.2)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: nbclassic in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: jupyterlab-server~=2.10 in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (2.16.3)\n",
            "Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (6.2)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (3.6.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.15.0)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.7.0)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (7.4.8)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: send2trash in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (21.3.0)\n",
            "Requirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (7.2.6)\n",
            "Requirement already satisfied: traitlets>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.6.0)\n",
            "Requirement already satisfied: jupyter-events>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.5.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 1)) (2.5.4)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: jsonschema[format-nongpl]>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (4.3.3)\n",
            "\u001b[33mWARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\u001b[0m\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.3.0->jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.3.0->jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.3.0->jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.19.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema[format-nongpl]>=4.3.0->jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.11.0)\n",
            "Requirement already satisfied: json5 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.9.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (4.13.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (4.6.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.0.1)\n",
            "Requirement already satisfied: mistune<3,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.16.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from nbclassic->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.28.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.9.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.9.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (4.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (14.0.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.19.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (2.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.2.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.9 MB 248 kB/s \n",
            "\u001b[?25hRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.8/dist-packages (from babel->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.18.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->jupyterlab->-r requirements.txt (line 1)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->jupyterlab->-r requirements.txt (line 1)) (3.0.9)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.22.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install numpy --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "JB8taZnkIzRX",
        "outputId": "cd49f26a-ad33-40d3-c5e3-ed727f786081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.22.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.5 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js5tqPsKTX1J"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "def print_line(*args):\n",
        "    \"\"\" Inline print and go to the begining of line\n",
        "    \"\"\"\n",
        "    args1 = [str(arg) for arg in args]\n",
        "    str_ = ' '.join(args1)\n",
        "    print('\\r' + str_, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piv5q63LTX1L",
        "outputId": "a2ca5fee-ad50-4085-812e-2c19a8e0092a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# If you are going to use GPU, make sure the GPU in in the output\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pv95c9Wc-GR"
      },
      "source": [
        "## Dependency Parsing\n",
        "\n",
        "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and words which modify those heads. There are multiple types of dependency parsers, including transition-based parsers, graph-based parsers, and feature-based parsers. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:\n",
        "* A *stack* of words that are currently being processed\n",
        "* A *buffer* of words yet to be processed.\n",
        "* A list of *dependencies* predicted by the parser.\n",
        "\n",
        "\n",
        "Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer is empty and the stack size is 1. The following transitions can be applied:\n",
        "* **SHIFT**: removes the first word from the buffer and pushes it onto the stack.\n",
        "* **LEFT-ARC**: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a first word → second word dependency to the dependency list.\n",
        "* **RIGHT-ARC**: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a second word → first word dependency to the dependency list.\n",
        "\n",
        "On each step, your parser will decide among the three transitions using a neural network classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf7B_vlmdx1Y"
      },
      "source": [
        "## 1. Transition Mechanics (60 points)\n",
        "In this section, you need to implement the transition mechanics your parser will use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSprrIJyTX1O"
      },
      "source": [
        "### 1.1 and 1.2 are written questions, please check the pdf handout for the details. (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EddMIWhSjytm"
      },
      "source": [
        "### 1.3 Parsing (Fill in the code, 20 points)\n",
        "\n",
        "There are two functions\n",
        "1. \\_\\_init\\_\\_() (10 points)\n",
        "2. parse_step() (10 points)\n",
        "\n",
        "Please follow the comments and fill in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN9ZNcnReHmG"
      },
      "outputs": [],
      "source": [
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
        "                                        Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### Start your code\n",
        "        ### Your code should initialize the following fields:\n",
        "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
        "        ###                 last element of the list.\n",
        "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
        "        ###                  buffer as the first item of the list\n",
        "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "        ###             tuples where each tuple is of the form (head, dependent).\n",
        "        ###             Order for this list doesn't matter.\n",
        "        ###\n",
        "        ### Note: The root token should be represented with the string \"ROOT\"\n",
        "        ### Note: If you need to use the sentence object to initialize anything, make sure to not directly \n",
        "        ###       reference the sentence object.  That is, remember to NOT modify the sentence object. \n",
        "        \n",
        "        self.stack = [\"ROOT\"]\n",
        "        self.buffer = []\n",
        "        for sent in self.sentence:\n",
        "          self.buffer.append(sent)\n",
        "        self.dependencies = []\n",
        "        ### End\n",
        "\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
        "                                left-arc, and right-arc transitions. You can assume the provided\n",
        "                                transition is a legal transition.\n",
        "        \"\"\"\n",
        "        ### Start your code\n",
        "        ### TODO:\n",
        "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
        "        ###     described above:\n",
        "        ###         1. Shift\n",
        "        ###         2. Left Arc\n",
        "        ###         3. Right Arc\n",
        "\n",
        "        if transition == \"S\":\n",
        "            t = self.buffer[0]\n",
        "            self.stack.append(t)\n",
        "            self.buffer.pop(0)\n",
        "        elif transition == \"LA\":\n",
        "            self.dependencies.append((self.stack[-1],self.stack[-2]))\n",
        "            self.stack.pop(-2)\n",
        "        else:\n",
        "            self.dependencies.append((self.stack[-2],self.stack[-1]))\n",
        "            self.stack.pop(-1)\n",
        "\n",
        "        ### End\n",
        "        \n",
        "    def should_stop(self):\n",
        "        return len(self.buffer) == 0 and len(self.stack) <= 1\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
        "\n",
        "        @return dependencies (list of string tuples): The list of dependencies produced when\n",
        "                                                        parsing the sentence. Represented as a list of\n",
        "                                                        tuples where each tuple is of the form (head, dependent).\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi_bM9cOfuP7"
      },
      "source": [
        "Execute the following cell to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNzqAqc3f3Nl",
        "outputId": "614288f0-82e8-461d-e33a-2f5d902af2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHIFT test passed!\n",
            "LEFT-ARC test passed!\n",
            "RIGHT-ARC test passed!\n",
            "parse test passed!\n"
          ]
        }
      ],
      "source": [
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))\n",
        "\n",
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
        "    \n",
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")\n",
        "\n",
        "test_parse_step()\n",
        "test_parse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMHunGyxf-lm"
      },
      "source": [
        "### 1.4 Minibatch Dependecy Parsing (Fill in the code, 20 points)\n",
        "\n",
        "Since neural networks run much more efficiently when making predictions about batches of data at a time, in this section, you need to implement this Minibatch algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qikoL9AhPwO"
      },
      "outputs": [],
      "source": [
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    @param sentences (list of list of str): A list of sentences to be parsed\n",
        "                                            (each sentence is a list of words and each word is of type string)\n",
        "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
        "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "                                returns a list of transitions predicted for each parse. That is, after calling\n",
        "                                    transitions = model.predict(partial_parses)\n",
        "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
        "\n",
        "\n",
        "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
        "                                                    list for a parsed sentence. Ordering should be the\n",
        "                                                    same as in sentences (i.e., dependencies[i] should\n",
        "                                                    contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "    dependencies = []\n",
        "\n",
        "    ### Start to code\n",
        "    ### TODO:\n",
        "    ###     Implement the minibatch parse algorithm.\n",
        "    ###     1. Build a parser list for all sentences\n",
        "    ###     2. Build an unfinished parser list that equals to the parser list\n",
        "    ###     3. Run a while loop when the unfinished_parser list is not empty\n",
        "    ###     4.     In the while loop, retrieve the first batch of the parser list\n",
        "    ###     5.     Use mode.predict to predict transition strings for this batch.\n",
        "    ###     6.     for every parser in this batch, do the parse in the PartialParse class with the predicted transition\n",
        "    ###     7.     Iterate all parser and only keep parser that should not stop in the unfinished parser list\n",
        "    ###     8. After all parser is finished, retrieve the dependencies\n",
        "\n",
        "    partial_parses = [PartialParse(sent) for sent in sentences]\n",
        "    unfinished_parse = partial_parses\n",
        "\n",
        "    while len(unfinished_parse) > 0:\n",
        "        minibatch = unfinished_parse[0:batch_size]        \n",
        "        while len(minibatch) > 0:\n",
        "            transitions = model.predict(minibatch)\n",
        "            for index, action in enumerate(transitions):\n",
        "                minibatch[index].parse_step(action)\n",
        "            minibatch = [parse for parse in minibatch if len(parse.stack) > 1 or len(parse.buffer) > 0]        \n",
        "        unfinished_parse = unfinished_parse[batch_size:]\n",
        "\n",
        "    dependencies = []\n",
        "    \n",
        "    for n in range(len(sentences)):\n",
        "        dependencies.append(partial_parses[n].dependencies)\n",
        "\n",
        "    ### End\n",
        "\n",
        "    return dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E9mhBJ3hxDl"
      },
      "source": [
        "Run the following cell to test your implementation.\n",
        "\n",
        "**Note:** You will need minibatch parse to be correctly implemented to evaluate the model you will build in the next sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJHg3KAnh9la",
        "outputId": "f4edf327-c0db-46cc-ee5e-8f1563d268f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minibatch_parse test passed!\n"
          ]
        }
      ],
      "source": [
        "class DummyModel(object):\n",
        "    \"\"\"Dummy model for testing the minibatch_parse function\n",
        "    \"\"\"\n",
        "    def __init__(self, mode = \"unidirectional\"):\n",
        "        self.mode = mode\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        if self.mode == \"unidirectional\":\n",
        "            return self.unidirectional_predict(partial_parses)\n",
        "        elif self.mode == \"interleave\":\n",
        "            return self.interleave_predict(partial_parses)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def unidirectional_predict(self, partial_parses):\n",
        "        \"\"\"First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
        "        the sentence is \"right\", \"left\" if otherwise.\n",
        "        \"\"\"\n",
        "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "    def interleave_predict(self, partial_parses):\n",
        "        \"\"\"First shifts everything onto the stack and then interleaves \"right\" and \"left\".\n",
        "        \"\"\"\n",
        "        return [(\"RA\" if len(pp.stack) % 2 == 0 else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "def test_dependencies(name, deps, ex_deps):\n",
        "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
        "    deps = tuple(sorted(deps))\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "\n",
        "\n",
        "def test_minibatch_parse():\n",
        "    \"\"\"Simple tests for the minibatch_parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "\n",
        "    # Unidirectional arcs test\n",
        "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "                 [\"left\", \"arcs\", \"only\"],\n",
        "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    \n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[1],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[2],\n",
        "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[3],\n",
        "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
        "\n",
        "    # Out-of-bound test\n",
        "    sentences = [[\"right\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0], (('ROOT', 'right'),))\n",
        "\n",
        "    # Mixed arcs test\n",
        "    sentences = [[\"this\", \"is\", \"interleaving\", \"dependency\", \"test\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(mode=\"interleave\"), 1)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'is'), ('dependency', 'interleaving'),\n",
        "                      ('dependency', 'test'), ('is', 'dependency'), ('is', 'this')))\n",
        "    print(\"minibatch_parse test passed!\")\n",
        "\n",
        "test_minibatch_parse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_JqUE-NkOM8"
      },
      "source": [
        "## 2. Neural Networks for parsing (40 points)\n",
        "\n",
        "In this section, you are going to build and train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyfzMmYVvJgx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import ReLU, Softmax\n",
        "from tensorflow.keras.initializers import GlorotUniform, RandomUniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8HyxtLHkcuZ"
      },
      "source": [
        "## 2.1 Create your model (fill the code, 20 points)\n",
        "The input of model is a list of integers $\\mathbf{w} =[w_1, w_2, ..., w_m]$ where $m$ is the number of features. Then our network looks up an embedding for each word and concatenates them into a single input vector:\n",
        "\n",
        "$\\mathbf{x} = [\\mathbf{E}_{w_1}, \\mathbf{E}_{w_2}, ..., \\mathbf{E}_{w_m}]$\n",
        "\n",
        "where $\\mathbf{E}\\in \\mathbb{R}^{|V|\\times d}$ is an embedding matrix with each row $\\mathbf{E}_w$ as the vector for a particular word $w$.\n",
        "\n",
        "Then, we compuate our prediction as:\n",
        "\n",
        "> $\\mathbf{h} = ReLU(xW + b_1)$\n",
        "\n",
        "> $\\mathbf{l} = hU + b_2$\n",
        "\n",
        "> $\\hat{y} = softmax(l)$\n",
        "\n",
        "where $\\mathbf{h}$ is referred to as the hidden layer, $\\mathbf{l}$ is referred to as the logits, $\\hat{y}$ is referred to as the predictions, and $ReLU(z)=max(z, 0)$.\n",
        "\n",
        "We will train the model to minimize cross-entropty loss:\n",
        "\n",
        "> $J(\\theta)=CE(y, \\hat{y})=-\\sum_{i=1}^3 y_i \\log \\hat{y}_i$\n",
        "\n",
        "To compute the loss for the training set, we average this J(θ) across all training examples.\n",
        "We will use UAS score as our evaluation metric. UAS refers to Unlabeled Attachment Score, which is computed as the ratio between number of correctly predicted dependencies and the number of total dependencies despite of the relations (our model doesn’t predict this).\n",
        "\n",
        "**Note:**\n",
        "To test your understanding of embedding lookup, so **DO NOT** use any high level API like **tf.keras.layers.Embedding** and **tf.keras.layers.Dense** in your code, otherwise you will receive deductions.\n",
        "\n",
        "**Hints:**\n",
        "* Each of the variables you are asked to declare (self.embed to hidden weight, self.embed to hidden bias, self.hidden to logits weight, self.hidden to logits bias) corresponds to one of the variables above (W, b1, U, b2).\n",
        "* It should take about 1 hour to train the model on the entire the training dataset, i.e., when debug mode is disabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAVwU01ulIjU"
      },
      "outputs": [],
      "source": [
        "class ParserModel(Model):\n",
        "    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n",
        "    The ParserModel will predict which transition should be applied to a\n",
        "    given partial parse configuration.\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, n_features=36, hidden_size=200, n_classes=3):\n",
        "        \"\"\" Initialize the parser model.\n",
        "\n",
        "        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n",
        "        @param n_features (int): number of input features\n",
        "        @param hidden_size (int): number of hidden units\n",
        "        @param n_classes (int): number of output classes\n",
        "        \"\"\"\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embeddings = tf.convert_to_tensor(embeddings, dtype=tf.float32)\n",
        "\n",
        "        ### Start your code\n",
        "        ### TODO:\n",
        "        ###     1) Declare `self.w` and `self.b1` using `self.add_weight`.\n",
        "        ###        Initialize weight with the `GlorotUniform` function and bias with `RandomUniform`\n",
        "        ###        with default parameters.\n",
        "        ###     2) Declare `self.u` and `self.b2` using `self.add_weight`.\n",
        "        ###        Initialize weight with the `GlorotUniform` function and bias with `RandomUniform`\n",
        "        ###        with default parameters.\n",
        "        ###     3) Declare `self.relu`\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     add_weight: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight\n",
        "        ###     Initialization: https://www.tensorflow.org/api_docs/python/tf/keras/initializers\n",
        "        ### \n",
        "\n",
        "        self.w = self.add_weight(name = 'input_weight', shape=(self.embed_size*self.n_features, self.hidden_size), initializer = GlorotUniform, trainable = True)\n",
        "        self.b1 = self.add_weight(name = 'input_bias',shape=(1, self.hidden_size), initializer = RandomUniform, trainable = True)\n",
        "        \n",
        "        self.u = self.add_weight(name = 'hidden_weight', shape=(self.hidden_size, self.n_classes), initializer=GlorotUniform, trainable=True)\n",
        "        self.b2 = self.add_weight(name = 'hidden_bias', shape=(1, self.n_classes), initializer=RandomUniform, trainable= True)\n",
        "        \n",
        "        self.relu = ReLU()\n",
        "\n",
        "        ### End\n",
        "\n",
        "    def embedding_lookup(self, w):\n",
        "        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n",
        "            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n",
        "\n",
        "            @return x (Tensor): tensor of embeddings for words represented in w\n",
        "                                (batch_size, n_features * embed_size)\n",
        "        \"\"\"\n",
        "\n",
        "        ### Start your code\n",
        "        ### TODO:\n",
        "        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n",
        "        ###     2) Reshape the tensor using `tf.reshape` to concatenates them into a single vector\n",
        "\n",
        "        x = tf.reshape(tf.gather(self.embeddings, w), (w.shape[0], w.shape[1] * self.embed_size))\n",
        "\n",
        "        ### End\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "    def call(self, w):\n",
        "        \"\"\" Run the model forward.\n",
        "\n",
        "            Note that we will not apply the softmax function here because we will use logits in the loss function\n",
        "\n",
        "        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
        "                                 without applying softmax (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        ### Start your code\n",
        "        ### TODO:\n",
        "        ###     Complete the forward computation as described in write-up.\n",
        "\n",
        "        x = self.embedding_lookup(w)\n",
        "        h = self.relu(tf.linalg.matmul(x, self.w) + self.b1)\n",
        "        logits = tf.linalg.matmul(h, self.u) + self.b2\n",
        "\n",
        "        ### End\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06yMPybqTX1X"
      },
      "source": [
        "Run the follwoing cell to test your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqOLP639m_kA",
        "outputId": "5e3af8d8-e49a-4ac9-bf61-6483a136a785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding_lookup sanity check passes!\n",
            "Forward sanity check passes!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(6666)\n",
        "embeddings = np.random.randn(100, 30)\n",
        "model = ParserModel(embeddings)\n",
        "\n",
        "def check_embedding():\n",
        "    inds = tf.random.uniform((4, 36), 0, 100, dtype=tf.int64)\n",
        "    selected = model.embedding_lookup(inds)\n",
        "    assert (selected.numpy() - embeddings[inds].reshape((4, -1))).sum() < 1e-5, \"The result of embedding lookup: \" \\\n",
        "                                                                                + repr(selected) \\\n",
        "                                                                                + \" does not match the original embeddings.\"\n",
        "\n",
        "def check_forward():\n",
        "    inputs = tf.random.uniform((4, 36), 0, 100, dtype=tf.int64)\n",
        "    out = model(inputs)\n",
        "    expected_out_shape = (4, 3)\n",
        "    assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n",
        "                                            \" which doesn't match expected \" + repr(expected_out_shape)\n",
        "\n",
        "check_embedding()\n",
        "print(\"Embedding_lookup sanity check passes!\")\n",
        "\n",
        "check_forward()\n",
        "print(\"Forward sanity check passes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTa0evHldfT"
      },
      "source": [
        "### 2.2 Training (Fill the code, 20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkAHYd3_le93"
      },
      "outputs": [],
      "source": [
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \"\"\" Train the neural dependency parser.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param output_path (str): Path to which model weights and results are written.\n",
        "    @param batch_size (int): Number of examples in a single batch\n",
        "    @param n_epochs (int): Number of training epochs\n",
        "    @param lr (float): Learning rate\n",
        "    \"\"\"\n",
        "    best_dev_UAS = 0\n",
        "\n",
        "\n",
        "    ### Start your code\n",
        "    ### TODO:\n",
        "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
        "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n",
        "    ###         reduction (default)\n",
        "    ###\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate= lr)\n",
        "    loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    ### END\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            parser.model.save_weights(output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \"\"\" Train the neural dependency parser for single epoch.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
        "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
        "    @param batch_size (int): batch size\n",
        "\n",
        "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
        "    \"\"\"\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "        if i % 10 == 0 or i == n_minibatches - 1:\n",
        "            print_line(f'Step {i + 1} / {n_minibatches}')\n",
        "        loss = 0. # store loss for this batch here\n",
        "        train_x = tf.convert_to_tensor(train_x, dtype=tf.int64)\n",
        "        train_y = tf.convert_to_tensor(train_y, dtype=tf.int64)\n",
        "\n",
        "        ### Start your code\n",
        "        ### TODO:\n",
        "        ###      1) Run train_x forward through model to produce `logits`\n",
        "        ###      2) Use the `loss_func` parameter to apply the CrossEntropyLoss function.\n",
        "        ###         This will take `train_y` and `logits` as inputs. It will output the CrossEntropyLoss\n",
        "        ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
        "        ###         are the predictions (y^ from the PDF).\n",
        "        ###      3) Backprop losses\n",
        "        ###      4) Take step with the optimizer\n",
        "        ### You can refer to our previous Assignment:\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          output = parser.model(train_x)\n",
        "          loss = loss_func(train_y, output)\n",
        "          train_vars = parser.model.trainable_variables\n",
        "          gradients = tape.gradient(loss, train_vars)\n",
        "          optimizer.apply_gradients(zip(gradients, train_vars))\n",
        "\n",
        "        # output = parser.model(train_x)\n",
        "        # loss = loss_func(train_y, output)\n",
        "        # train_vars = parser.model.trainable_variables\n",
        "        # gradients = tf.GradientTape().gradient(loss, train_vars)\n",
        "        # optimizer.apply_gradients(zip(gradients, train_vars))\n",
        "\n",
        "        ### End\n",
        "\n",
        "        loss_meter.update(loss.numpy())\n",
        "    print('\\n')\n",
        "\n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcvOHj36TX1Y"
      },
      "source": [
        "Run the following cell to train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj_KcipUl5X1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c350e00e-eb69-4728-85f6-6e84c07eefd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "INITIALIZING\n",
            "================================================================================\n",
            "Loading data...\n",
            "took 4.50 seconds\n",
            "Building parser...\n",
            "took 2.21 seconds\n",
            "Loading pretrained embeddings...\n",
            "took 2.20 seconds\n",
            "Vectorizing data...\n",
            "took 2.07 seconds\n",
            "Preprocessing training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 39832/39832 [00:46<00:00, 854.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 46.65 seconds\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
        "\n",
        "debug = False ## Set to True if you want to debug your code\n",
        "\n",
        "print(80 * \"=\")\n",
        "print(\"INITIALIZING\")\n",
        "print(80 * \"=\")\n",
        "parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(minibatch_parse, debug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ifgGzamM6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01e7577-b415-4a65-925e-0754ab9fe8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 0.01 seconds\n",
            "\n",
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.15447135853431956\n",
            "Evaluating on dev set\n",
            "- dev UAS: 83.40\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.09652193447220184\n",
            "Evaluating on dev set\n",
            "- dev UAS: 85.71\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.08456325242739349\n",
            "Evaluating on dev set\n",
            "- dev UAS: 86.33\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.07704085248091068\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.22\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.07144228303684043\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.17\n",
            "\n",
            "Epoch 6 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.06701429121728454\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.43\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.06344716526463112\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.40\n",
            "\n",
            "Epoch 8 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.06012106317325265\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.62\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.057285796699730995\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.53\n",
            "\n",
            "Epoch 10 out of 10\n",
            "Step 1848 / 1848\n",
            "\n",
            "Average Train Loss: 0.05446295733301541\n",
            "Evaluating on dev set\n",
            "- dev UAS: 87.71\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n",
            "- test UAS: 88.24\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "seed = 6669\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "start = time.time()\n",
        "model = ParserModel(embeddings)\n",
        "parser.model = model\n",
        "print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "\n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
        "\n",
        "if not debug:\n",
        "    print(80 * \"=\")\n",
        "    print(\"TESTING\")\n",
        "    print(80 * \"=\")\n",
        "    print(\"Restoring the best model weights found on the dev set\")\n",
        "    parser.model.load_weights(output_path)\n",
        "    print(\"Final evaluation on test set\",)\n",
        "    UAS, dependencies = parser.parse(test_data)\n",
        "    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buzfT0GVTX1c"
      },
      "source": [
        "If you implement correctly, the dev UAS will be about 88. The training loss will be about 0.06"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conlusion: dev UAS is 87.71, Average Loss is 0.054, test dev UAS is 88.24.**"
      ],
      "metadata": {
        "id": "EOeRBcGPTQ1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tlm8ymfTX1c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}